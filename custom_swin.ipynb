{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "from torchvision.ops import StochasticDepth, Permute, MLP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import ImageNet, CocoDetection\n",
    "\n",
    "from typing import Tuple, Callable, List, Optional\n",
    "import lightning.pytorch as pl\n",
    "from torchsummary import summary\n",
    "import torchmetrics\n",
    "import net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─SwinTransformer: 1-1                   [-1, 15, 15, 768]         --\n",
      "|    └─ImagePartition: 2-1               [-1, 120, 120, 96]        --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 96, 120, 120]        4,704\n",
      "|    |    └─Permute: 3-2                 [-1, 120, 120, 96]        --\n",
      "|    |    └─LayerNorm: 3-3               [-1, 120, 120, 96]        192\n",
      "|    └─ModuleList: 2                     []                        --\n",
      "|    |    └─Sequential: 3-4              [-1, 120, 120, 96]        229,830\n",
      "|    |    └─PatchMerging: 3-5            [-1, 60, 60, 192]         74,112\n",
      "|    |    └─Sequential: 3-6              [-1, 60, 60, 192]         898,956\n",
      "|    |    └─PatchMerging: 3-7            [-1, 30, 30, 384]         295,680\n",
      "|    |    └─Sequential: 3-8              [-1, 30, 30, 384]         10,692,936\n",
      "|    |    └─PatchMerging: 3-9            [-1, 15, 15, 768]         1,181,184\n",
      "|    |    └─Sequential: 3-10             [-1, 15, 15, 768]         14,203,440\n",
      "├─ClassificationHead: 1-2                [-1, 1000]                --\n",
      "|    └─LayerNorm: 2-2                    [-1, 15, 15, 768]         1,536\n",
      "|    └─Permute: 2-3                      [-1, 768, 15, 15]         --\n",
      "|    └─AdaptiveAvgPool2d: 2-4            [-1, 768, 1, 1]           --\n",
      "|    └─Flatten: 2-5                      [-1, 768]                 --\n",
      "|    └─Linear: 2-6                       [-1, 1000]                769,000\n",
      "==========================================================================================\n",
      "Total params: 28,351,570\n",
      "Trainable params: 28,351,570\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 150.46\n",
      "==========================================================================================\n",
      "Input size (MB): 2.64\n",
      "Forward/backward pass size (MB): 40.88\n",
      "Params size (MB): 108.15\n",
      "Estimated Total Size (MB): 151.67\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swin = net.LitSwin.swin_t()\n",
    "\n",
    "summary(swin, (3, 480, 480))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83429506\n"
     ]
    }
   ],
   "source": [
    "swin = net.LitSwin.s()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# summary(swin, (3, 480, 480))\n",
    "# print()\n",
    "print(count_parameters(swin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "83,429,506\n",
    "102,233,026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 8, 12])\n",
      "torch.Size([3, 12, 8, 8])\n",
      "torch.Size([3, 24, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge = net.PatchMerging(12)\n",
    "x = torch.rand((3, 7, 7, 12))\n",
    "merge(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8, 8, 12])\n",
      "num_windows 4\n",
      "torch.Size([8, 2, 4, 2, 4, 12])\n",
      "torch.Size([32, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "b, d = 8, 12\n",
    "attention = net.SwinAttention(\n",
    "    d, [4, 4], [2, 2], 1, 0.1, 0.1\n",
    ")  # attention layer with cyclic shift\n",
    "image = torch.rand(b, 7, 7, d)\n",
    "\n",
    "out = attention(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relative_position_bias(\n",
    "    relative_position_bias_table: torch.Tensor, relative_position_index: torch.Tensor, window_size: List[int]\n",
    ") -> torch.Tensor:\n",
    "    N = window_size[0] * window_size[1]\n",
    "    relative_position_bias = relative_position_bias_table[relative_position_index]  # type: ignore[index]\n",
    "    relative_position_bias = relative_position_bias.view(N, N, -1)\n",
    "    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)\n",
    "    return relative_position_bias\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(2 * dim)  # difference\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        H, W, _ = x.shape[-3:]\n",
    "        x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x0 = x[..., 0::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x1 = x[..., 1::2, 0::2, :]  # ... H/2 W/2 C\n",
    "        x2 = x[..., 0::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x3 = x[..., 1::2, 1::2, :]  # ... H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # ... H/2 W/2 4*C\n",
    "        \n",
    "        x = self.reduction(x)  # ... H/2 W/2 2*C\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        dim: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        num_heads: int,\n",
    "        attention_dropout: float,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.query = self.build_query()\n",
    "        self.key = self.build_key()\n",
    "        self.value = self.build_value()\n",
    "        self.linear = self.build_linear()\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.define_relative_position_bias_table()\n",
    "        self.define_relative_position_index()\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n",
    "        )\n",
    "\n",
    "    def build_query(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def build_key(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def build_value(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def build_linear(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def _logits(self): ...\n",
    "\n",
    "    def _attention(self): ...\n",
    "    \n",
    "    def _post_attention(self): ...\n",
    "\n",
    "    def define_relative_position_bias_table(self):\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n",
    "        relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n",
    "\n",
    "        relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n",
    "        relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n",
    "\n",
    "        relative_coords_table *= 8  # normalize to -8, 8\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / 3.0\n",
    "        )\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "    def define_relative_position_index(self):\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1).flatten()  # Wh*Ww*Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def get_relative_position_bias(self) -> torch.Tensor:\n",
    "        relative_position_bias = _get_relative_position_bias(\n",
    "            self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads),\n",
    "            self.relative_position_index,  # type: ignore[arg-type]\n",
    "            self.window_size,\n",
    "        )\n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        return relative_position_bias\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, H, W, C = x.size()\n",
    "        window_size = self.window_size\n",
    "        pad_r = (window_size[1] - W % window_size[1]) % window_size[1]\n",
    "        pad_b = (window_size[0] - H % window_size[0]) % window_size[0]\n",
    "        x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "        _, pad_H, pad_W, _ = x.shape\n",
    "\n",
    "        shift_size = self.shift_size.copy()\n",
    "        # If window size is larger than feature size, there is no need to shift window\n",
    "        if self.window_size[0] >= pad_H:\n",
    "            shift_size[0] = 0\n",
    "        if self.window_size[1] >= pad_W:\n",
    "            shift_size[1] = 0\n",
    "        # cyclic shift\n",
    "        if sum(shift_size) > 0:\n",
    "            x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "        # partition windows\n",
    "        num_windows = (pad_H // window_size[0]) * (pad_W // window_size[1])\n",
    "        x = x.view(B, pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1], C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, window_size[0] * window_size[1], C)  # B*nW, Ws*Ws, C\n",
    "        \n",
    "        q, k, v = self.query(x), self.key(x), self.query(x)\n",
    "        q = q.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=math.log(100.0)).exp()\n",
    "        attn = attn * logit_scale\n",
    "        attn = attn + self.get_relative_position_bias()\n",
    "        \n",
    "        if sum(shift_size) > 0:\n",
    "            # generate attention mask\n",
    "            attn_mask = x.new_zeros((pad_H, pad_W))\n",
    "            h_slices = ((0, -self.window_size[0]), (-self.window_size[0], -shift_size[0]), (-shift_size[0], None))\n",
    "            w_slices = ((0, -self.window_size[1]), (-self.window_size[1], -shift_size[1]), (-shift_size[1], None))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    attn_mask[h[0] : h[1], w[0] : w[1]] = count\n",
    "                    count += 1\n",
    "            attn_mask = attn_mask.view(pad_H // self.window_size[0], self.window_size[0], pad_W // self.window_size[1], self.window_size[1])\n",
    "            attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, self.window_size[0] * self.window_size[1])\n",
    "            attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "            attn = attn.view(x.size(0) // num_windows, num_windows, self.num_heads, x.size(1), x.size(1))\n",
    "            attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, x.size(1), x.size(1))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # reverse windows\n",
    "        x = x.view(B, pad_H // self.window_size[0], pad_W // self.window_size[1], self.window_size[0], self.window_size[1], C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if sum(shift_size) > 0:\n",
    "            x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "\n",
    "        # unpad features\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        mlp_ratio: float,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        stochastic_depth_prob: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-5)\n",
    "        self.attn = SwinAttention(\n",
    "            dim,\n",
    "            window_size,\n",
    "            shift_size,\n",
    "            num_heads,\n",
    "            attention_dropout=attention_dropout,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-5)\n",
    "        self.mlp = MLP(dim, [int(dim * mlp_ratio), dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.mlp.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = x + self.stochastic_depth(self.norm1(self.attn(x)))\n",
    "        x = x + self.stochastic_depth(self.norm2(self.mlp(x)))\n",
    "        return x\n",
    "    \n",
    "class SwinTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        patch_size: Tuple[int, int],\n",
    "        window_size: Tuple[int, int],\n",
    "        embed_dim: int,\n",
    "        depths: List[int],\n",
    "        num_heads: List[int],\n",
    "        attention_dropout: float, dropout: float,\n",
    "        stochastic_depth_prob: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.tokenizer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                3, embed_dim,\n",
    "                kernel_size=(patch_size[0], patch_size[1]),\n",
    "                stride=(patch_size[0], patch_size[1])\n",
    "            ),\n",
    "            Permute([0, 2, 3, 1]),  # B C W H -> B W H C\n",
    "            partial(nn.LayerNorm, eps=1e-5)(embed_dim),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        total_stage_blocks = sum(depths)\n",
    "        stage_block_id = 0\n",
    "\n",
    "        for i_stage in range(len(depths)):\n",
    "            stage: List[nn.Module] = []\n",
    "            dim = embed_dim * 2**i_stage\n",
    "            for i_layer in range(depths[i_stage]):\n",
    "                # adjust stochastic depth probability based on the depth of the stage block\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
    "                stage.append(\n",
    "                    SwinBlock(\n",
    "                        dim,\n",
    "                        num_heads[i_stage],\n",
    "                        window_size=window_size,\n",
    "                        shift_size=[0 if i_layer % 2 == 0 else w // 2 for w in window_size],\n",
    "                        mlp_ratio=4.0,\n",
    "                        dropout=dropout,\n",
    "                        attention_dropout=attention_dropout,\n",
    "                        stochastic_depth_prob=sd_prob,\n",
    "                        # norm_layer=partial(nn.LayerNorm, eps=1e-5),\n",
    "                    )\n",
    "                )\n",
    "                stage_block_id += 1\n",
    "            self.layers.append(nn.Sequential(*stage))\n",
    "            # add patch merging layer\n",
    "            if i_stage < (len(depths) - 1):\n",
    "                self.layers.append(PatchMerging(dim))\n",
    "        # self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            print(layer, x.shape)\n",
    "        return x\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(num_features, eps=1e-5)\n",
    "        self.permute = Permute([0, 3, 1, 2])  # B H W C -> B C H W\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.permute(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "    \n",
    "class LitSwin(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "        patch_size: Tuple[int, int],\n",
    "        window_size: Tuple[int, int],\n",
    "        embed_dim: int,\n",
    "        depths: List[int],\n",
    "        num_heads: List[int],\n",
    "        attention_dropout: float, dropout: float,\n",
    "        stochastic_depth_prob: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.swin = SwinTransformer(\n",
    "            patch_size, window_size,\n",
    "            embed_dim, depths, num_heads,\n",
    "            attention_dropout, dropout,\n",
    "            stochastic_depth_prob\n",
    "        )\n",
    "        self.head = ClassificationHead(embed_dim * 2 ** (len(depths) - 1), 1000)\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.swin(x))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        features = self.swin(x)\n",
    "        logits = self.head(features)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        features = self.swin(x)\n",
    "        logits = self.head(features)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        self.accuracy.update(logits, y)\n",
    "        # self.log('valid_acc', self.accuracy, on_step=True, on_epoch=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('val_acc', self.accuracy.compute())\n",
    "        self.accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    # def _transfer_parameter(self, base, target):\n",
    "\n",
    "    @classmethod\n",
    "    def swin_t(cls):\n",
    "        model = swin_v2_t(Swin_V2_T_Weights.IMAGENET1K_V1)\n",
    "        swin = cls(\n",
    "            patch_size=[4, 4],\n",
    "            embed_dim=96,\n",
    "            depths=[2, 2, 6, 2],\n",
    "            num_heads=[3, 6, 12, 24],\n",
    "            window_size=[8, 8],\n",
    "            attention_dropout=0.1, dropout=0.1,\n",
    "            stochastic_depth_prob=0.2\n",
    "        )\n",
    "        # print(swin)\n",
    "        \n",
    "        layers = list(model.children())\n",
    "        # s = layers[0]\n",
    "        # for layer in s.children():\n",
    "        #     print(layer)\n",
    "        #     print('-'*30)\n",
    "\n",
    "        for base, target in zip(swin.head.children(), layers[1:]):\n",
    "            base.load_state_dict(target.state_dict())\n",
    "\n",
    "        for base, target in zip(list(swin.swin.children())[0], list(layers[0].children())[0]):\n",
    "            base.load_state_dict(target.state_dict())\n",
    "\n",
    "        for base, target in zip(list(swin.swin.children())[1][1::2], list(layers[0].children())[2::2]):\n",
    "            base.reduction.load_state_dict(target.reduction.state_dict())\n",
    "            base.norm.load_state_dict(target.norm.state_dict())\n",
    "\n",
    "        for base, target in zip(list(swin.swin.children())[1][0::2], list(layers[0].children())[1::2]):\n",
    "            # base.load_state_dict(target.state_dict())\n",
    "            for b, t in zip(list(base.children()), list(target.children())):\n",
    "                dim = t.attn.qkv.weight.shape[0]\n",
    "                d = dim // 3\n",
    "                # print(b.attn.query.weight.shape, t.attn.qkv.weight[0:d].clone().shape)\n",
    "                # print(b.attn.key.weight.shape, t.attn.qkv.weight[d:2*d].clone().shape)\n",
    "                # print(b.attn.value.weight.shape, t.attn.qkv.weight[-d:].clone().shape)\n",
    "                b.attn.query.weight = nn.Parameter(t.attn.qkv.weight[0:d].clone())\n",
    "                b.attn.key.weight = nn.Parameter(t.attn.qkv.weight[d:d*2].clone())\n",
    "                b.attn.value.weight = nn.Parameter(t.attn.qkv.weight[-d:].clone())\n",
    "                b.attn.query.bias = nn.Parameter(t.attn.qkv.bias[0:d].clone())\n",
    "                b.attn.key.bias = nn.Parameter(t.attn.qkv.bias[d:d*2].clone())\n",
    "                b.attn.value.bias = nn.Parameter(t.attn.qkv.bias[-d:].clone())\n",
    "                # print(t.attn.query)\n",
    "                b.norm1.load_state_dict(t.norm1.state_dict())\n",
    "                b.norm2.load_state_dict(t.norm2.state_dict())\n",
    "                b.mlp.load_state_dict(t.mlp.state_dict())\n",
    "                # b.attn.cpb_mlp.load_state_dict(t.attn.cpb_mlp.state_dict())\n",
    "                b.attn.linear.load_state_dict(t.attn.proj.state_dict())\n",
    "                b.attn.cpb_mlp.load_state_dict(t.attn.cpb_mlp.state_dict())\n",
    "                b.attn.logit_scale = nn.Parameter(t.attn.logit_scale.clone())\n",
    "\n",
    "                b.attn.relative_position_index = t.attn.relative_position_index.clone()\n",
    "                b.attn.relative_coords_table = t.attn.relative_coords_table.clone()\n",
    "\n",
    "        return model, swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = swin_v2_t(\n",
    "    Swin_V2_T_Weights.IMAGENET1K_V1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, swin = LitSwin.swin_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin.eval()\n",
    "swin(torch.randn(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(list(model.children())[0].children())[2::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(swin.swin.children())[1][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CocoSample:\n",
    "    segmentation: List[float]\n",
    "    area: float\n",
    "    iscrowd: bool\n",
    "    image_id: int\n",
    "    bbox: Tuple[float]\n",
    "    category_id: int\n",
    "    id_: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_annotation(cls, anno):\n",
    "        return cls(\n",
    "            id_=anno['id'], image_id=anno['image_id'], category_id =anno['category_id'],\n",
    "            segmentation=anno['segmentation'], area=anno['area'], iscrowd=anno['iscrowd'], bbox=anno['bbox'],\n",
    "        )\n",
    "    \n",
    "\n",
    "def transform(image):\n",
    "    print(image)\n",
    "    return image\n",
    "ds = CocoDetection(\n",
    "    '/media/leonard/data/dataset/coco/val2017/',\n",
    "    '/media/leonard/data/dataset/coco/annotations/instances_val2017.json',\n",
    "    transform=Swin_T_Weights.DEFAULT.transforms(),\n",
    "    target_transform=lambda anno: [CocoSample.from_annotation(x) for x in anno]\n",
    ")\n",
    "\n",
    "for image, anno in ds:\n",
    "    # print(image.shape)\n",
    "    for a in anno:\n",
    "        print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        attention_dropout: float,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.query = self.build_query()\n",
    "        self.key = self.build_key()\n",
    "        self.value = self.build_value()\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def build_query(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def build_key(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def build_value(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def _logits(self): ...\n",
    "\n",
    "    def _attention(self): ...\n",
    "    \n",
    "    def _post_attention(self): ...\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, H, W, C = x.size()\n",
    "        q = self.query()\n",
    "\n",
    "\n",
    "def shifted_window_attention(\n",
    "    input: Tensor,\n",
    "    qkv_weight: Tensor,\n",
    "    proj_weight: Tensor,\n",
    "    relative_position_bias: Tensor,\n",
    "    window_size: List[int],\n",
    "    num_heads: int,\n",
    "    shift_size: List[int],\n",
    "    attention_dropout: float = 0.0,\n",
    "    dropout: float = 0.0,\n",
    "    qkv_bias: Optional[Tensor] = None,\n",
    "    proj_bias: Optional[Tensor] = None,\n",
    "    logit_scale: Optional[torch.Tensor] = None,\n",
    "    training: bool = True,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        input (Tensor[N, H, W, C]): The input tensor or 4-dimensions.\n",
    "        qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value.\n",
    "        proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection.\n",
    "        relative_position_bias (Tensor): The learned relative position bias added to attention.\n",
    "        window_size (List[int]): Window size.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        shift_size (List[int]): Shift size for shifted window attention.\n",
    "        attention_dropout (float): Dropout ratio of attention weight. Default: 0.0.\n",
    "        dropout (float): Dropout ratio of output. Default: 0.0.\n",
    "        qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None.\n",
    "        proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None.\n",
    "        logit_scale (Tensor[out_dim], optional): Logit scale of cosine attention for Swin Transformer V2. Default: None.\n",
    "        training (bool, optional): Training flag used by the dropout parameters. Default: True.\n",
    "    Returns:\n",
    "        Tensor[N, H, W, C]: The output tensor after shifted window attention.\n",
    "    \"\"\"\n",
    "    B, H, W, C = input.shape\n",
    "    # pad feature maps to multiples of window size\n",
    "    pad_r = (window_size[1] - W % window_size[1]) % window_size[1]\n",
    "    pad_b = (window_size[0] - H % window_size[0]) % window_size[0]\n",
    "    x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))\n",
    "    _, pad_H, pad_W, _ = x.shape\n",
    "\n",
    "    shift_size = shift_size.copy()\n",
    "    # If window size is larger than feature size, there is no need to shift window\n",
    "    if window_size[0] >= pad_H:\n",
    "        shift_size[0] = 0\n",
    "    if window_size[1] >= pad_W:\n",
    "        shift_size[1] = 0\n",
    "\n",
    "    # cyclic shift\n",
    "    if sum(shift_size) > 0:\n",
    "        x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "\n",
    "    # partition windows\n",
    "    num_windows = (pad_H // window_size[0]) * (pad_W // window_size[1])\n",
    "    x = x.view(B, pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, window_size[0] * window_size[1], C)  # B*nW, Ws*Ws, C\n",
    "\n",
    "    # multi-head attention\n",
    "    if logit_scale is not None and qkv_bias is not None:\n",
    "        qkv_bias = qkv_bias.clone()\n",
    "        length = qkv_bias.numel() // 3\n",
    "        qkv_bias[length : 2 * length].zero_()\n",
    "    qkv = F.linear(x, qkv_weight, qkv_bias)\n",
    "    qkv = qkv.reshape(x.size(0), x.size(1), 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "    if logit_scale is not None:\n",
    "        # cosine attention\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)\n",
    "        logit_scale = torch.clamp(logit_scale, max=math.log(100.0)).exp()\n",
    "        attn = attn * logit_scale\n",
    "    else:\n",
    "        q = q * (C // num_heads) ** -0.5\n",
    "        attn = q.matmul(k.transpose(-2, -1))\n",
    "    # add relative position bias\n",
    "    attn = attn + relative_position_bias\n",
    "\n",
    "    if sum(shift_size) > 0:\n",
    "        # generate attention mask\n",
    "        attn_mask = x.new_zeros((pad_H, pad_W))\n",
    "        h_slices = ((0, -window_size[0]), (-window_size[0], -shift_size[0]), (-shift_size[0], None))\n",
    "        w_slices = ((0, -window_size[1]), (-window_size[1], -shift_size[1]), (-shift_size[1], None))\n",
    "        count = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                attn_mask[h[0] : h[1], w[0] : w[1]] = count\n",
    "                count += 1\n",
    "        attn_mask = attn_mask.view(pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1])\n",
    "        attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, window_size[0] * window_size[1])\n",
    "        attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        attn = attn.view(x.size(0) // num_windows, num_windows, num_heads, x.size(1), x.size(1))\n",
    "        attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)\n",
    "        attn = attn.view(-1, num_heads, x.size(1), x.size(1))\n",
    "\n",
    "    attn = F.softmax(attn, dim=-1)\n",
    "    attn = F.dropout(attn, p=attention_dropout, training=training)\n",
    "\n",
    "    x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)\n",
    "    x = F.linear(x, proj_weight, proj_bias)\n",
    "    x = F.dropout(x, p=dropout, training=training)\n",
    "\n",
    "    # reverse windows\n",
    "    x = x.view(B, pad_H // window_size[0], pad_W // window_size[1], window_size[0], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)\n",
    "\n",
    "    # reverse cyclic shift\n",
    "    if sum(shift_size) > 0:\n",
    "        x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "\n",
    "    # unpad features\n",
    "    x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "class ShiftWindowAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: List[int],\n",
    "        shift_size: List[int],\n",
    "        qkv_bias: bool = True,\n",
    "        proj_bias: bool = True,\n",
    "        attention_dropout: float=0.1,\n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.query = self.build_query()\n",
    "        self.key = self.build_key()\n",
    "        self.value = self.build_value()\n",
    "        self.linear = self.build_linear()\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n",
    "        )\n",
    "\n",
    "    def build_query(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def build_key(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "    \n",
    "    def build_value(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def build_linear(self):\n",
    "        return nn.Linear(self.dim, self.dim)\n",
    "\n",
    "    def _logits(self): ...\n",
    "\n",
    "    def _attention(self): ...\n",
    "    \n",
    "    def _post_attention(self): ...\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, H, W, C = x.size()\n",
    "\n",
    "        pad_r = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]\n",
    "        pad_b = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]\n",
    "        x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "        _, pad_H, pad_W, _ = x.shape\n",
    "\n",
    "        shift_size = self.shift_size.copy()\n",
    "        # If window size is larger than feature size, there is no need to shift window\n",
    "        if self.window_size[0] >= pad_H:\n",
    "            shift_size[0] = 0\n",
    "        if self.window_size[1] >= pad_W:\n",
    "            shift_size[1] = 0\n",
    "\n",
    "        # cyclic shift\n",
    "        if sum(shift_size) > 0:\n",
    "            x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "        num_windows = (pad_H // self.window_size[0]) * (pad_W // self.window_size[1])\n",
    "        x = x.view(B, pad_H // self.window_size[0], self.window_size[0], pad_W // self.window_size[1], self.window_size[1], C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, self.window_size[0] * self.window_size[1], C)  # B*nW, Ws*Ws, C\n",
    "\n",
    "        q, k, v = self.query(x), self.key(x), self.query(x)\n",
    "        \n",
    "        # qkv = qkv.reshape(x.size(0), x.size(1), 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q = q.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(x.size(0), x.size(1), self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=math.log(100.0)).exp()\n",
    "        attn = attn * logit_scale\n",
    "\n",
    "        if sum(shift_size) > 0:\n",
    "            # generate attention mask\n",
    "            attn_mask = x.new_zeros((pad_H, pad_W))\n",
    "            h_slices = ((0, -self.window_size[0]), (-self.window_size[0], -shift_size[0]), (-shift_size[0], None))\n",
    "            w_slices = ((0, -self.window_size[1]), (-self.window_size[1], -shift_size[1]), (-shift_size[1], None))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    attn_mask[h[0] : h[1], w[0] : w[1]] = count\n",
    "                    count += 1\n",
    "            attn_mask = attn_mask.view(pad_H // self.window_size[0], self.window_size[0], pad_W // self.window_size[1], self.window_size[1])\n",
    "            attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, self.window_size[0] * self.window_size[1])\n",
    "            attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "            attn = attn.view(x.size(0) // num_windows, num_windows, self.num_heads, x.size(1), x.size(1))\n",
    "            attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, x.size(1), x.size(1))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # reverse windows\n",
    "        x = x.view(B, pad_H // self.window_size[0], pad_W // self.window_size[1], self.window_size[0], self.window_size[1], C)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if sum(shift_size) > 0:\n",
    "            x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "\n",
    "        # unpad features\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: int,\n",
    "        shift_size: int,\n",
    "        mlp_ratio: float,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        stochastic_depth_prob: float,\n",
    "        attn_layer: Callable[..., nn.Module], # = ShiftedWindowAttention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = attn_layer(\n",
    "            dim,\n",
    "            window_size,\n",
    "            shift_size,\n",
    "            num_heads,\n",
    "            attention_dropout=attention_dropout,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, [int(dim * mlp_ratio), dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "        \n",
    "        for m in self.mlp.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, patch_size: Tuple[int, int]):\n",
    "        super().__init__()\n",
    "        self.partition = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                3, embed_dim, kernel_size=(patch_size[0], patch_size[1]), stride=(patch_size[0], patch_size[1])\n",
    "            ),\n",
    "            Permute([0, 2, 3, 1]),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "\n",
    "    def fowrad(self, image):\n",
    "        patches = self.partition(image)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features: int,  num_classes: int):\n",
    "        super().__init__()\n",
    "        # num_features = embed_dim * 2 ** (len(depths) - 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, feature_map):\n",
    "        feature = self.flatten(self.avg_pool(feature_map))\n",
    "        logits = self.head(feature)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = ShiftWindowAttention(96, 12, (4, 4), [2, 2])\n",
    "attn_layer(torch.randn(1, 31, 31, 96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = self.build_layer()\n",
    "\n",
    "    def build_layer(self):\n",
    "        return nn.Conv2d(3, 10, (3, 3))\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.layer(image)\n",
    "\n",
    "layer = Layer().cuda()\n",
    "print(layer)\n",
    "\n",
    "image = torch.rand((4, 3, 12, 12))\n",
    "\n",
    "layer(image.cuda()).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f8e845ca4ef26093e116a69c0c0225ebcc6616c4e8131964bb48d1688e6a5fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
